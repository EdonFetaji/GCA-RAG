{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Check GPU"
      ],
      "metadata": {
        "id": "e_2rOjTRCuGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "2uEbLKtCCtcq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Deps"
      ],
      "metadata": {
        "id": "9_RqReHfCwPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt update -y -qq\n",
        "!apt install -y -qq curl lshw libcairo2-dev pkg-config python3-dev\n",
        "!curl https://ollama.ai/install.sh | sh\n",
        "\n",
        "!pip install flask -q\n",
        "!pip install pyngrok -q\n",
        "!pip install requests -q\n",
        "!pip install flask-cors -q\n",
        "!apt update -y -qq\n",
        "!apt install -y -qq curl lshw libcairo2-dev pkg-config python3-dev zstd\n",
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "vKClg4iV3tnN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "\n",
        "Please set the NGROK auth token to access the tunnel."
      ],
      "metadata": {
        "id": "kY-BMho4soa9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_AUTH_TOKEN = '' #@param {type:'string'}\n",
        "OLLAMA_URL = 'http://127.0.0.1:11434' #@param {type:'string'}"
      ],
      "metadata": {
        "id": "8WIVDY-V-kVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Ollama Service\n",
        "\n",
        "This cell starts the Ollama service and pulls the default model."
      ],
      "metadata": {
        "id": "start_ollama"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "import time\n",
        "import requests\n",
        "\n",
        "ollama_process = subprocess.Popen(\n",
        "    [\"ollama\", \"serve\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.PIPE\n",
        ")\n",
        "\n",
        "print(\"Waiting for Ollama to start...\")\n",
        "max_attempts = 30\n",
        "for i in range(max_attempts):\n",
        "    try:\n",
        "        response = requests.get(f'{OLLAMA_URL}/api/tags', timeout=1)\n",
        "        if response.status_code == 200:\n",
        "            print(\"‚úì Ollama service is ready!\")\n",
        "            break\n",
        "    except:\n",
        "        pass\n",
        "    time.sleep(1)\n",
        "    if i == max_attempts - 1:\n",
        "        print(\"‚ö† Warning: Ollama may not have started properly\")\n",
        "\n",
        "print(\"Recommended models:\\n\")\n",
        "print(\"  ‚Ä¢ llama3.1:8b      - Fast, good quality (4.7GB)\")\n",
        "print(\"  ‚Ä¢ mistral:7b       - Fast, efficient (4.1GB)\")\n",
        "print(\"  ‚Ä¢ gemma2:9b        - Very good quality (5.5GB)\")\n",
        "print(\"  ‚Ä¢ qwen2.5:7b       - Great for coding (4.7GB)\")\n",
        "print(\"  ‚Ä¢ llama3.2:3b      - Ultra fast, smaller (2GB)\")\n",
        "\n",
        "!ollama pull gemma2:9b\n",
        "!ollama pull llama3.1:8b\n",
        "!ollama pull zephyr\n",
        "\n",
        "print(\"\\n‚úì Model ready!\")\n",
        "print(\"\\nTo pull additional models later, use:\")\n",
        "print(\"  !ollama pull <model-name>\")"
      ],
      "metadata": {
        "id": "start_ollama_service"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Code"
      ],
      "metadata": {
        "id": "U4I-fc5kCKyS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, Response, jsonify\n",
        "import json\n",
        "import subprocess\n",
        "import requests\n",
        "from pyngrok import ngrok\n",
        "from flask_cors import CORS\n",
        "import os\n",
        "from urllib.parse import urlencode"
      ],
      "metadata": {
        "id": "xJpGDX7t6LA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "app = Flask(__name__)\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "CORS(app)\n",
        "\n",
        "@app.route('/api/generate', methods=['POST'])\n",
        "def generate_completion():\n",
        "    try:\n",
        "        # Get data from JSON body (more standard than form data)\n",
        "        data = request.get_json() if request.is_json else {}\n",
        "\n",
        "        # Fallback to form data if no JSON\n",
        "        model = data.get('model') or request.form.get('model') or 'zephyr'\n",
        "        prompt = data.get('prompt') or request.form.get('prompt')\n",
        "        persona = data.get('persona') or request.form.get('persona') or \"You are 2B from NieR Automata. Answer as 2B, the assistant, only.\"\n",
        "        temperature = data.get('temperature') or request.form.get('temperature') or 0.8\n",
        "\n",
        "        if not prompt:\n",
        "            return jsonify({\"error\": \"No prompt provided\"}), 400\n",
        "\n",
        "        json_data = {\n",
        "            \"model\": model,\n",
        "            \"prompt\": prompt,\n",
        "            \"system\": persona,\n",
        "            \"options\": {\"temperature\": float(temperature)},\n",
        "            \"stream\": False\n",
        "        }\n",
        "\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "        response = requests.post(f'{OLLAMA_URL}/api/generate', json=json_data, headers=headers, timeout=120)\n",
        "\n",
        "        return jsonify(response.json())\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return jsonify({\"error\": \"Cannot connect to Ollama service. Make sure it's running.\"}), 503\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/api/pull', methods=['POST'])\n",
        "def pull_model():\n",
        "    try:\n",
        "        data = request.get_json() if request.is_json else {}\n",
        "        model_name = data.get('name') or request.form.get('name')\n",
        "\n",
        "        if not model_name:\n",
        "            return jsonify({\"error\": \"No model name provided\"}), 400\n",
        "\n",
        "        json_data = {\"name\": model_name, \"stream\": False}\n",
        "        headers = {\"Content-Type\": \"application/json\"}\n",
        "        response = requests.post(f'{OLLAMA_URL}/api/pull', json=json_data, headers=headers, timeout=300)\n",
        "\n",
        "        return jsonify(response.json())\n",
        "\n",
        "    except requests.exceptions.ConnectionError:\n",
        "        return jsonify({\"error\": \"Cannot connect to Ollama service. Make sure it's running.\"}), 503\n",
        "    except Exception as e:\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "@app.route('/api/tags', methods=['GET'])\n",
        "def list_models():\n",
        "    \"\"\"List available models\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f'{OLLAMA_URL}/api/tags', timeout=10)\n",
        "        return jsonify(response.json())\n",
        "    except:\n",
        "        return jsonify({\"error\": \"Cannot connect to Ollama service\"}), 503\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f'{OLLAMA_URL}/api/tags', timeout=5)\n",
        "        if response.status_code == 200:\n",
        "            return jsonify({\"status\": \"healthy\", \"ollama\": \"connected\"})\n",
        "    except:\n",
        "        pass\n",
        "    return jsonify({\"status\": \"unhealthy\", \"ollama\": \"disconnected\"}), 503\n",
        "\n",
        "# Start ngrok tunnel\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STARTING NGROK TUNNEL\")\n",
        "print(\"=\"*60)\n",
        "http_tunnel = ngrok.connect(5000)\n",
        "public_url = http_tunnel.public_url\n",
        "print(f\"\\nüåê Public URL: {public_url}\")\n",
        "print(f\"\\nüìù Add this to your .env file:\")\n",
        "print(f\"   OLLAMA_BASE_URL={public_url}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Available endpoints:\")\n",
        "print(f\"  ‚Ä¢ POST {public_url}/api/generate\")\n",
        "print(f\"  ‚Ä¢ POST {public_url}/api/pull\")\n",
        "print(f\"  ‚Ä¢ GET  {public_url}/api/tags\")\n",
        "print(f\"  ‚Ä¢ GET  {public_url}/health\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "# Run Flask app\n",
        "app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "wx4M5serF7NR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h2CguZ0OMMzX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}